{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('segment guns.v2i.coco-segmentation/train/_annotations.coco.json', 'r') as file:\n",
    "    data = json.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T12:56:33.177110Z",
     "start_time": "2023-10-17T12:56:33.065445400Z"
    }
   },
   "id": "b01fec0ab1dbd662"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def read_dataset(path, n=-1):\n",
    "    features, labels = [], []\n",
    "    transform = transforms.ToTensor()\n",
    "    for i, image in enumerate(data['images']):\n",
    "        if i == n: break\n",
    "        features.append(transform(Image.open(os.path.join(path, str(image['file_name'])))))\n",
    "        annotations = list(filter(lambda annotation: annotation.get('image_id') == image['id'], data['annotations']))\n",
    "        label = np.zeros((image['height'], image['width']), dtype=np.uint8)\n",
    "        for annotation in annotations:\n",
    "            pts = np.array(annotation.get('segmentation'), dtype=np.int32)\n",
    "            pts = pts.reshape((1, int(pts.shape[-1] / 2), 2))\n",
    "            cv2.fillPoly(label, pts, 1)\n",
    "\n",
    "        labels.append(torch.from_numpy(label))\n",
    "\n",
    "    return features, labels\n",
    "path = 'C:\\\\Proxectos\\\\Custom_U-NET\\\\segment guns.v2i.coco-segmentation\\\\train'\n",
    "\n",
    "# A, B = read_dataset(path, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T12:56:33.184038800Z",
     "start_time": "2023-10-17T12:56:33.183534700Z"
    }
   },
   "id": "d01543cb7645cade"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def voc_rand_crop(feature, label, height, width):\n",
    "    \"\"\"Randomly crop both feature and label images.\"\"\"\n",
    "    rect = torchvision.transforms.RandomCrop.get_params(\n",
    "        feature, (height, width))\n",
    "    feature = torchvision.transforms.functional.crop(feature, *rect)\n",
    "    label = torchvision.transforms.functional.crop(label, *rect)\n",
    "    return feature, label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T12:56:33.188943100Z",
     "start_time": "2023-10-17T12:56:33.184038800Z"
    }
   },
   "id": "5592e80cbe874c1a"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
    "    \"\"\"Plot a list of images.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        try:\n",
    "            img = img.detach().numpy()\n",
    "        except:\n",
    "            pass\n",
    "        ax.imshow(img)\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes\n",
    "     \n",
    "# imgs = []\n",
    "# n=5\n",
    "# for _ in range(n):\n",
    "#     imgs += voc_rand_crop(A[3], B[3].unsqueeze(0) , 420, 600)\n",
    "# \n",
    "# imgs = [img.permute(1, 2, 0) for img in imgs]\n",
    "# show_images(imgs[::2] + imgs[1::2], 2, n, scale = 10);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T12:56:33.194383900Z",
     "start_time": "2023-10-17T12:56:33.188943100Z"
    }
   },
   "id": "6513accb5b27540d"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class GunsDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, crop_size, path):\n",
    "        self.transform = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.crop_size = crop_size\n",
    "        features, labels = read_dataset(path)\n",
    "        self.features = [self.normalize_image(feature)\n",
    "                         for feature in self.filter(features)]\n",
    "        self.labels = self.filter(labels)\n",
    "        print('read ' + str(len(self.features)) + ' examples')\n",
    "\n",
    "    def normalize_image(self, img):\n",
    "        return self.transform(img.float() / 255.0)\n",
    "\n",
    "    def filter(self, imgs):\n",
    "        return [img for img in imgs if (\n",
    "            img.shape[1] >= self.crop_size[0] and\n",
    "            img.shape[2] >= self.crop_size[1])]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature, label = voc_rand_crop(self.features[idx], self.labels[idx], *self.crop_size)\n",
    "        return feature, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "     "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T12:56:33.201588300Z",
     "start_time": "2023-10-17T12:56:33.194383900Z"
    }
   },
   "id": "d989c4f13f900f5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crop_size = (420, 600)\n",
    "voc_train = GunsDataset(crop_size, path)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-17T12:56:33.201588300Z"
    }
   },
   "id": "cb4a116fb5e748f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=True,\n",
    "                                    drop_last=True,\n",
    "                                    num_workers=4)\n",
    "\n",
    "for X, Y in train_iter:\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2efb4be54ce14b06"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
